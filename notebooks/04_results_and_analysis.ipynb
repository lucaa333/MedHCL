{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04. Results and Analysis - 3D CNN Architecture Comparison\n",
        "\n",
        "## Comparative Analysis of 3D CNN Architectures\n",
        "\n",
        "This notebook compares different 3D CNN architectures for medical image classification:\n",
        "\n",
        "- **ResNet-18 (3D)** - Recommended default (~33M params)\n",
        "- **ResNet-34 (3D)** - More capacity (~63M params)\n",
        "- **ResNet-50 (3D)** - Maximum performance (~46M params)\n",
        "- **DenseNet-121 (3D)** - Efficient feature reuse (~5.6M params)\n",
        "- **EfficientNet-B0 (3D)** - Most parameter efficient (~1.2M params)\n",
        "\n",
        "**Research Question:** How do different 3D CNN architectures perform on hierarchical medical image classification?\n",
        "\n",
        "**Prerequisites:** Run notebooks 02 and 03 first to train models and generate checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "from utils.cnn_3d_models import get_3d_model\n",
        "from utils.data_loader import get_medmnist_dataloaders\n",
        "from utils.metrics import evaluate_model\n",
        "from config import *\n",
        "\n",
        "# Set style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "print(f\"Device: {DEVICE}\")\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: /home/luca/.medmnist/organmnist3d.npz\n",
            "Using downloaded and verified file: /home/luca/.medmnist/organmnist3d.npz\n",
            "Using downloaded and verified file: /home/luca/.medmnist/organmnist3d.npz\n",
            "‚úì Test set loaded: 610 samples\n",
            "‚úì Number of classes: 11\n",
            "‚úì Output directories ready\n"
          ]
        }
      ],
      "source": [
        "# Load test dataset\n",
        "_, _, test_loader, num_classes = get_medmnist_dataloaders(\n",
        "    dataset_name='organ',\n",
        "    batch_size=32,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "print(f\"‚úì Test set loaded: {len(test_loader.dataset)} samples\")\n",
        "print(f\"‚úì Number of classes: {num_classes}\")\n",
        "\n",
        "# Create output directories\n",
        "Path('../results').mkdir(exist_ok=True)\n",
        "Path('../figures').mkdir(exist_ok=True)\n",
        "print(\"‚úì Output directories ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Architectures to Compare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Architectures to compare:\n",
            "  - ResNet-18            (resnet18_3d)\n",
            "  - ResNet-34            (resnet34_3d)\n",
            "  - ResNet-50            (resnet50_3d)\n",
            "  - DenseNet-121         (densenet121_3d)\n",
            "  - EfficientNet-B0      (efficientnet3d_b0)\n"
          ]
        }
      ],
      "source": [
        "# Architectures to compare\n",
        "ARCHITECTURES = {\n",
        "    'ResNet-18': 'resnet18_3d',\n",
        "    'ResNet-34': 'resnet34_3d',\n",
        "    'ResNet-50': 'resnet50_3d',\n",
        "    'DenseNet-121': 'densenet121_3d',\n",
        "    'EfficientNet-B0': 'efficientnet3d_b0'\n",
        "}\n",
        "\n",
        "print(\"Architectures to compare:\")\n",
        "for name, arch in ARCHITECTURES.items():\n",
        "    print(f\"  - {name:20s} ({arch})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Trained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 60.81 MiB is free. Process 13489 has 21.97 MiB memory in use. Process 19046 has 936.00 MiB memory in use. Process 23989 has 2.99 GiB memory in use. Including non-PyTorch memory, this process has 1.03 GiB memory in use. Process 35425 has 782.00 MiB memory in use. Process 36080 has 934.00 MiB memory in use. Process 37311 has 154.00 MiB memory in use. Of the allocated memory 840.95 MiB is allocated by PyTorch, and 63.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[39], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00march\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_baseline.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_3d_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43march\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Try to load checkpoint\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.11/lib/python3.12/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.11/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.11/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.11/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.11/lib/python3.12/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.11/lib/python3.12/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 60.81 MiB is free. Process 13489 has 21.97 MiB memory in use. Process 19046 has 936.00 MiB memory in use. Process 23989 has 2.99 GiB memory in use. Including non-PyTorch memory, this process has 1.03 GiB memory in use. Process 35425 has 782.00 MiB memory in use. Process 36080 has 934.00 MiB memory in use. Process 37311 has 154.00 MiB memory in use. Of the allocated memory 840.95 MiB is allocated by PyTorch, and 63.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "models = {}\n",
        "model_info = {}\n",
        "\n",
        "for name, arch in ARCHITECTURES.items():\n",
        "    checkpoint_path = Path(f'../models/{arch}_baseline.pth')\n",
        "    \n",
        "    # Create model\n",
        "    model = get_3d_model(arch, num_classes=num_classes).to(DEVICE)\n",
        "    params = sum(p.numel() for p in model.parameters())\n",
        "    \n",
        "    # Try to load checkpoint\n",
        "    if checkpoint_path.exists():\n",
        "        try:\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            model.eval()\n",
        "            \n",
        "            models[name] = model\n",
        "            model_info[name] = {\n",
        "                'architecture': arch,\n",
        "                'params': params,\n",
        "                'trained': True,\n",
        "                'val_acc': max(checkpoint.get('history', {}).get('val_acc', [0.0]))\n",
        "            }\n",
        "            print(f\"‚úÖ {name:20s} - Loaded ({params/1e6:.1f}M params)\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå {name:20s} - Error: {e}\")\n",
        "    else:\n",
        "        model.eval()\n",
        "        models[name] = model\n",
        "        model_info[name] = {\n",
        "            'architecture': arch,\n",
        "            'params': params,\n",
        "            'trained': False,\n",
        "            'val_acc': 0.0\n",
        "        }\n",
        "        print(f\"‚ö†Ô∏è  {name:20s} - Not trained ({params/1e6:.1f}M params)\")\n",
        "\n",
        "trained_count = sum(1 for info in model_info.values() if info['trained'])\n",
        "print(f\"\\n‚úì Loaded {len(models)} models ({trained_count} trained)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Information Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "MODEL SUMMARY\n",
            "================================================================================\n",
            "          Model      Architecture  Parameters (M) Trained Val Accuracy\n",
            "      ResNet-18       resnet18_3d       33.165643       ‚úó          N/A\n",
            "      ResNet-34       resnet34_3d       63.475275       ‚úó          N/A\n",
            "      ResNet-50       resnet50_3d       46.177611       ‚úó          N/A\n",
            "   DenseNet-121    densenet121_3d        5.584523       ‚úó          N/A\n",
            "EfficientNet-B0 efficientnet3d_b0        1.219595       ‚úó          N/A\n",
            "================================================================================\n",
            "\n",
            "‚ö†Ô∏è  WARNING: No trained models found!\n",
            "   Run notebooks 02 and 03 first to train models.\n"
          ]
        }
      ],
      "source": [
        "# Create summary table\n",
        "summary_data = []\n",
        "for name, info in model_info.items():\n",
        "    summary_data.append({\n",
        "        'Model': name,\n",
        "        'Architecture': info['architecture'],\n",
        "        'Parameters (M)': info['params'] / 1e6,\n",
        "        'Trained': '‚úì' if info['trained'] else '‚úó',\n",
        "        'Val Accuracy': f\"{info['val_acc']:.4f}\" if info['trained'] else 'N/A'\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(summary_df.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "if trained_count == 0:\n",
        "    print(\"\\n‚ö†Ô∏è  WARNING: No trained models found!\")\n",
        "    print(\"   Run notebooks 02 and 03 first to train models.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluate Models on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≠Ô∏è  Skipping ResNet-18 (not trained)\n",
            "‚è≠Ô∏è  Skipping ResNet-34 (not trained)\n",
            "‚è≠Ô∏è  Skipping ResNet-50 (not trained)\n",
            "‚è≠Ô∏è  Skipping DenseNet-121 (not trained)\n",
            "‚è≠Ô∏è  Skipping EfficientNet-B0 (not trained)\n",
            "\n",
            "‚ö†Ô∏è  No trained models to evaluate\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    if not model_info[name]['trained']:\n",
        "        print(f\"‚è≠Ô∏è  Skipping {name} (not trained)\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\nEvaluating {name}...\")\n",
        "    metrics, preds, labels = evaluate_model(model, test_loader, DEVICE)\n",
        "    \n",
        "    results[name] = {\n",
        "        'accuracy': metrics['accuracy'],\n",
        "        'precision': metrics['precision'],\n",
        "        'recall': metrics['recall'],\n",
        "        'f1_score': metrics['f1_score'],\n",
        "        'confusion_matrix': metrics['confusion_matrix'],\n",
        "        'per_class': metrics['per_class']\n",
        "    }\n",
        "    \n",
        "    print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
        "    print(f\"  F1-Score:  {metrics['f1_score']:.4f}\")\n",
        "\n",
        "if results:\n",
        "    print(f\"\\n‚úì Evaluated {len(results)} model(s)\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No trained models to evaluate\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Performance Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  No results to compare\n"
          ]
        }
      ],
      "source": [
        "if results:\n",
        "    # Create comparison table\n",
        "    comparison_data = []\n",
        "    for name in results.keys():\n",
        "        comparison_data.append({\n",
        "            'Model': name,\n",
        "            'Architecture': model_info[name]['architecture'],\n",
        "            'Params (M)': model_info[name]['params'] / 1e6,\n",
        "            'Accuracy': results[name]['accuracy'],\n",
        "            'Precision': results[name]['precision'],\n",
        "            'Recall': results[name]['recall'],\n",
        "            'F1-Score': results[name]['f1_score']\n",
        "        })\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"PERFORMANCE COMPARISON\")\n",
        "    print(\"=\"*100)\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    print(\"=\"*100)\n",
        "    \n",
        "    # Save\n",
        "    comparison_df.to_csv('../results/architecture_comparison.csv', index=False)\n",
        "    print(\"\\n‚úì Saved to '../results/architecture_comparison.csv'\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No results to compare\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  No results to visualize\n"
          ]
        }
      ],
      "source": [
        "if results:\n",
        "    # Performance bar chart\n",
        "    fig, ax = plt.subplots(figsize=(14, 6))\n",
        "    \n",
        "    metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "    x = np.arange(len(results))\n",
        "    width = 0.2\n",
        "    \n",
        "    for i, metric in enumerate(metrics_to_plot):\n",
        "        values = [results[name][metric.lower().replace('-', '_')] for name in results.keys()]\n",
        "        offset = (i - 1.5) * width\n",
        "        bars = ax.bar(x + offset, values, width, label=metric, alpha=0.8)\n",
        "        \n",
        "        # Add labels\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "    \n",
        "    ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Architecture Performance Comparison', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(results.keys(), rotation=15, ha='right')\n",
        "    ax.legend(fontsize=10)\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    ax.set_ylim([0, 1.05])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../figures/architecture_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"‚úì Saved to '../figures/architecture_comparison.png'\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No results to visualize\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  No results to visualize\n"
          ]
        }
      ],
      "source": [
        "if results:\n",
        "    # Accuracy vs Parameters\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    for i, name in enumerate(results.keys()):\n",
        "        params = model_info[name]['params'] / 1e6\n",
        "        acc = results[name]['accuracy']\n",
        "        ax.scatter(params, acc, s=300, alpha=0.7, label=name, edgecolors='black', linewidth=2)\n",
        "        ax.annotate(name, (params, acc), xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
        "    \n",
        "    ax.set_xlabel('Parameters (Millions)', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Accuracy vs Model Size', fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../figures/accuracy_vs_parameters.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"‚úì Saved to '../figures/accuracy_vs_parameters.png'\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No results to visualize\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  No results to visualize\n"
          ]
        }
      ],
      "source": [
        "if results:\n",
        "    # Best model confusion matrix\n",
        "    best_name = max(results.keys(), key=lambda k: results[k]['accuracy'])\n",
        "    best_result = results[best_name]\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "    \n",
        "    class_names = [ORGAN_CLASSES[i] for i in range(num_classes)]\n",
        "    sns.heatmap(\n",
        "        best_result['confusion_matrix'],\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names,\n",
        "        ax=ax,\n",
        "        cbar_kws={'label': 'Count'}\n",
        "    )\n",
        "    \n",
        "    ax.set_title(f\"Confusion Matrix - {best_name}\\nAccuracy: {best_result['accuracy']:.4f}\",\n",
        "                 fontsize=14, fontweight='bold', pad=20)\n",
        "    ax.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../figures/best_model_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"‚úì Confusion matrix for {best_name}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No results to visualize\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Per-Class Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  No results to analyze\n"
          ]
        }
      ],
      "source": [
        "if results:\n",
        "    best_name = max(results.keys(), key=lambda k: results[k]['accuracy'])\n",
        "    best_result = results[best_name]\n",
        "    \n",
        "    # Per-class F1 scores\n",
        "    per_class_df = pd.DataFrame({\n",
        "        'Organ': [ORGAN_CLASSES[i] for i in range(num_classes)],\n",
        "        'Region': [ORGAN_TO_REGION[ORGAN_CLASSES[i]] for i in range(num_classes)],\n",
        "        'F1-Score': best_result['per_class']['f1_score'],\n",
        "        'Support': best_result['per_class']['support']\n",
        "    })\n",
        "    per_class_df = per_class_df.sort_values('F1-Score', ascending=False)\n",
        "    \n",
        "    print(f\"\\nPer-Class Performance ({best_name}):\")\n",
        "    print(\"=\"*70)\n",
        "    print(per_class_df.to_string(index=False))\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Visualize\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    colors = [{'abdomen': '#FF6B6B', 'chest': '#4ECDC4', 'bone': '#45B7D1'}[r]\n",
        "              for r in per_class_df['Region']]\n",
        "    bars = ax.barh(per_class_df['Organ'], per_class_df['F1-Score'], color=colors, alpha=0.8)\n",
        "    \n",
        "    ax.set_xlabel('F1-Score', fontsize=12, fontweight='bold')\n",
        "    ax.set_title(f'Per-Class Performance - {best_name}', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlim([0, 1.05])\n",
        "    ax.grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()\n",
        "        ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
        "               f'{width:.3f}', ha='left', va='center', fontsize=9, fontweight='bold')\n",
        "    \n",
        "    # Legend\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [\n",
        "        Patch(facecolor='#FF6B6B', alpha=0.8, label='Abdomen'),\n",
        "        Patch(facecolor='#4ECDC4', alpha=0.8, label='Chest'),\n",
        "        Patch(facecolor='#45B7D1', alpha=0.8, label='Bone')\n",
        "    ]\n",
        "    ax.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../figures/per_class_performance.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"‚úì Saved to '../figures/per_class_performance.png'\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No results to analyze\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary and Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ö†Ô∏è  No trained models available\n",
            "   Run notebooks 02 and 03 first to train models\n"
          ]
        }
      ],
      "source": [
        "if results:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RESEARCH FINDINGS SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Best model\n",
        "    best_name = max(results.keys(), key=lambda k: results[k]['accuracy'])\n",
        "    best_result = results[best_name]\n",
        "    best_info = model_info[best_name]\n",
        "    \n",
        "    print(f\"\\nüìä BEST PERFORMING MODEL: {best_name}\")\n",
        "    print(f\"   Architecture: {best_info['architecture']}\")\n",
        "    print(f\"   Parameters: {best_info['params']/1e6:.2f}M\")\n",
        "    print(f\"   Test Accuracy: {best_result['accuracy']:.4f}\")\n",
        "    print(f\"   F1-Score: {best_result['f1_score']:.4f}\")\n",
        "    \n",
        "    # Most efficient\n",
        "    efficiency = {name: results[name]['accuracy'] / (model_info[name]['params'] / 1e6)\n",
        "                  for name in results.keys()}\n",
        "    most_efficient = max(efficiency.keys(), key=lambda k: efficiency[k])\n",
        "    \n",
        "    print(f\"\\n‚ö° MOST EFFICIENT MODEL: {most_efficient}\")\n",
        "    print(f\"   Accuracy per Million Parameters: {efficiency[most_efficient]:.4f}\")\n",
        "    \n",
        "    # All models\n",
        "    print(f\"\\nüìà ALL MODELS (sorted by accuracy):\")\n",
        "    for name in sorted(results.keys(), key=lambda k: results[k]['accuracy'], reverse=True):\n",
        "        acc = results[name]['accuracy']\n",
        "        params = model_info[name]['params'] / 1e6\n",
        "        print(f\"   {name:20s} - Acc: {acc:.4f}, Params: {params:6.2f}M\")\n",
        "    \n",
        "    print(\"\\n‚úÖ KEY FINDINGS:\")\n",
        "    print(\"   1. Evaluated multiple state-of-the-art 3D CNN architectures\")\n",
        "    print(\"   2. Analyzed accuracy vs efficiency trade-offs\")\n",
        "    print(\"   3. Identified optimal models for different use cases\")\n",
        "    \n",
        "    print(\"\\nüéØ RECOMMENDATIONS:\")\n",
        "    print(f\"   ‚Ä¢ For maximum accuracy: {best_name}\")\n",
        "    print(f\"   ‚Ä¢ For efficiency: {most_efficient}\")\n",
        "    print(\"   ‚Ä¢ For research: ResNet-18 provides good baseline\")\n",
        "    print(\"   ‚Ä¢ For deployment: Consider accuracy, speed, and memory\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"‚úì All results saved to '../results/' and '../figures/'\")\n",
        "    print(\"=\"*80)\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No trained models available\")\n",
        "    print(\"   Run notebooks 02 and 03 first to train models\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
